// HYBRID SPEED CRAWLER - Production-Ready with Ultimate Performance
// Combines CURL Multi-interface speed with enterprise-grade features
// Target: 300+ pages/sec with full compliance and robustness

#include "crawler_main.h"
#include "ultra_parser.h"
#include "language_detector.h"  // Add language detection
#include "constants.h"
#include <fstream>
#include <atomic>
#include <thread>
#include <csignal>
#include <iostream>
#include <mutex>
#include <algorithm>
#include <filesystem>
#include <unordered_set>
#include <queue>
#include <sstream>
#include <random>
#include <memory>
#include <iomanip>
#include <unordered_map>

// Write callback for CURL
size_t hybrid_write_callback(void* contents, size_t size, size_t nmemb, void* userp) {
    size_t total_size = size * nmemb;
    static_cast<std::string*>(userp)->append(static_cast<char*>(contents), total_size);
    return total_size;
}

// Global shutdown flag
std::atomic<bool> stop_flag{false};

// Global performance monitor
PerformanceMonitor global_monitor;

// Global components
std::unique_ptr<UrlFrontier> url_frontier;
std::unique_ptr<CrawlLogger> crawl_logger;
std::unique_ptr<FileStorageManager> file_storage;

/**
 * üîÑ DISK-BACKED URL QUEUE MANAGER
 * Provides persistent storage for URLs when memory queue is full
 */
class DiskBackedUrlManager {
private:
    std::string backlog_file_path_;
    std::mutex disk_mutex_;
    std::atomic<size_t> disk_queue_size_{0};
    
public:
    DiskBackedUrlManager(const std::string& file_path) : backlog_file_path_(file_path) {
        // Ensure directory exists
        std::filesystem::create_directories(std::filesystem::path(file_path).parent_path());
        
        // Count existing URLs in file
        count_existing_urls();
    }
    
    void count_existing_urls() {
        std::lock_guard<std::mutex> lock(disk_mutex_);
        std::ifstream file(backlog_file_path_);
        if (!file.is_open()) {
            disk_queue_size_ = 0;
            return;
        }
        
        size_t count = 0;
        std::string line;
        while (std::getline(file, line)) {
            if (!line.empty()) count++;
        }
        disk_queue_size_ = count;
    }
    
    // Save URLs to disk when memory queue is full
    bool save_urls_to_disk(const std::vector<std::string>& urls) {
        if (urls.empty()) return true;
        
        std::lock_guard<std::mutex> lock(disk_mutex_);
        std::ofstream file(backlog_file_path_, std::ios::app);
        if (!file.is_open()) {
            std::cerr << "Failed to open backlog file for writing: " << backlog_file_path_ << "\n";
            return false;
        }
        
        for (const auto& url : urls) {
            file << url << "\n";
        }
        
        disk_queue_size_ += urls.size();
        return true;
    }
    
    // Load URLs from disk to refill memory queue
    std::vector<std::string> load_urls_from_disk(size_t max_count = 5000) {
        std::lock_guard<std::mutex> lock(disk_mutex_);
        std::vector<std::string> loaded_urls;
        
        std::ifstream file(backlog_file_path_);
        if (!file.is_open()) return loaded_urls;
        
        std::string line;
        while (std::getline(file, line) && loaded_urls.size() < max_count) {
            if (!line.empty()) {
                loaded_urls.push_back(line);
            }
        }
        
        if (loaded_urls.empty()) return loaded_urls;
        
        // Rewrite file without the loaded URLs
        file.close();
        std::ifstream read_file(backlog_file_path_);
        std::vector<std::string> remaining_urls;
        
        size_t skip_count = loaded_urls.size();
        size_t current_count = 0;
        
        while (std::getline(read_file, line)) {
            if (!line.empty()) {
                if (current_count >= skip_count) {
                    remaining_urls.push_back(line);
                }
                current_count++;
            }
        }
        read_file.close();
        
        // Write remaining URLs back
        std::ofstream write_file(backlog_file_path_);
        for (const auto& url : remaining_urls) {
            write_file << url << "\n";
        }
        
        disk_queue_size_ = remaining_urls.size();
        return loaded_urls;
    }
    
    size_t get_disk_queue_size() const { return disk_queue_size_.load(); }
};

// Global disk-backed URL manager (Phase 2: Upgraded to sharded)
std::unique_ptr<ShardedDiskQueue> sharded_disk_queue;

// Global HTML processing pipeline (Phase 2)
std::unique_ptr<HtmlProcessingQueue> html_processing_queue;

// Global work stealing queue (Phase 2)
std::unique_ptr<WorkStealingQueue> work_stealing_queue;

/**
 * ü§ñ ADAPTIVE LINK EXTRACTOR
 * Dynamically adjusts link extraction based on page link density
 */
class AdaptiveLinkExtractor {
public:
    static std::vector<std::string> extract_links_adaptive(const std::string& html, const std::string& base_url) {
        // Use the ultra parser for maximum speed (300+ pages/sec)
        std::vector<std::string> all_links = HtmlParser::extract_links(html, base_url);
        std::vector<std::string> filtered_links;
        
        // Calculate adaptive extraction count
        size_t total_links = all_links.size();
        size_t extract_count;
        
        if (total_links <= CrawlerConstants::LinkExtraction::MIN_LINKS_FOR_EXTRACTION) {
            // Very few links, extract up to minimum
            extract_count = std::min(total_links, (size_t)CrawlerConstants::LinkExtraction::MIN_LINKS_EXTRACT);
        } else {
            // Many links, extract percentage but at least min, max configured amount
            size_t percentage_count = total_links * CrawlerConstants::LinkExtraction::EXTRACTION_PERCENTAGE;
            extract_count = std::min((size_t)CrawlerConstants::LinkExtraction::MAX_LINKS_EXTRACT, 
                                   std::max((size_t)CrawlerConstants::LinkExtraction::MIN_LINKS_EXTRACT, 
                                          percentage_count));
        }
        
        // Filter and collect valid links
        for (const auto& link : all_links) {
            if (filtered_links.size() >= extract_count) break;
            
            if (ContentFilter::is_crawlable_url(link)) {
                filtered_links.push_back(link);
            }
        }
        
        return filtered_links;
    }
    
    // Enhanced link processing with Phase 2 sharded disk fallback
    static int process_and_enqueue_links(const std::vector<std::string>& links, 
                                       int current_depth, 
                                       const std::string& referring_domain,
                                       size_t worker_id) {
        int successfully_enqueued = 0;
        std::vector<std::string> failed_urls;
        
        for (const std::string& link : links) {
            float priority = ContentFilter::calculate_priority(link, current_depth + 1);
            UrlInfo new_url_info(link, priority, current_depth + 1, referring_domain);
            
            // Try main frontier first
            if (url_frontier->enqueue(new_url_info)) {
                successfully_enqueued++;
            } 
            // Try work stealing queue for this worker
            else if (work_stealing_queue->push_local(worker_id, new_url_info)) {
                successfully_enqueued++;
            }
            else {
                // Both queues full, save to sharded disk
                failed_urls.push_back(link);
            }
        }
        
        // Save failed URLs to sharded disk
        if (!failed_urls.empty()) {
            sharded_disk_queue->save_urls_to_disk(failed_urls);
        }
        
        return successfully_enqueued;
    }
};
struct MultiRequestContext {
    CURL* curl_handle;
    UrlInfo url_info;
    std::string url;
    std::string response_data;
    std::string domain;
    std::chrono::steady_clock::time_point start_time;
    
    MultiRequestContext(const UrlInfo& info, ConnectionPool* pool = nullptr) 
        : url_info(info), url(info.url), domain(UrlNormalizer::extract_domain(info.url))
        , start_time(std::chrono::steady_clock::now()) {
        // Use domain-specific connection pooling if available
        if (pool) {
            curl_handle = pool->acquire_for_domain(domain);
        } else {
            curl_handle = curl_easy_init();
        }
        response_data.reserve(1024 * 1024); // 1MB pre-allocation
    }
    
    ~MultiRequestContext() {
        if (curl_handle) {
            curl_easy_cleanup(curl_handle);
        }
    }
};

/**
 * üåê DYNAMIC DOMAIN DISCOVERY MANAGER
 * Handles robots.txt fetching for newly discovered domains
 */
class DynamicDomainManager {
private:
    std::unordered_set<std::string> discovered_domains_;
    std::mutex domains_mutex_;
    
public:
    bool is_new_domain(const std::string& domain) {
        std::lock_guard<std::mutex> lock(domains_mutex_);
        return discovered_domains_.find(domain) == discovered_domains_.end();
    }
    
    void register_domain(const std::string& domain, RobotsTxtCache& robots) {
        std::lock_guard<std::mutex> lock(domains_mutex_);
        if (discovered_domains_.find(domain) == discovered_domains_.end()) {
            discovered_domains_.insert(domain);
            
            // Asynchronously fetch robots.txt for new domains
            std::thread([domain, &robots]() {
                try {
                    robots.fetch_and_cache(domain);
                    // Robots.txt fetched silently for performance
                } catch (...) {
                    // Silently ignore robots.txt fetch failures
                }
            }).detach();
        }
    }
    
    size_t get_discovered_count() const {
        std::lock_guard<std::mutex> lock(const_cast<std::mutex&>(domains_mutex_));
        return discovered_domains_.size();
    }
};

// Global domain manager
std::unique_ptr<DynamicDomainManager> domain_manager;

/**
 * High-performance multi-interface crawler worker
 */
void multi_crawler_worker(int worker_id, RobotsTxtCache& robots, RateLimiter& limiter,
                         DomainBlacklist& blacklist, ErrorTracker& error_tracker) {
    
    // Initialize connection pool for domain-specific connections
    ConnectionPool domain_connection_pool;
    
    // Initialize CURL multi handle
    CURLM* multi_handle = curl_multi_init();
    if (!multi_handle) {
        std::cerr << "Failed to initialize CURL multi handle in worker " << worker_id << "\n";
        return;
    }
    
    // Configure multi handle for maximum performance
    curl_multi_setopt(multi_handle, CURLMOPT_MAXCONNECTS, CrawlerConstants::Network::MAX_CONNECTIONS);
    curl_multi_setopt(multi_handle, CURLMOPT_MAX_TOTAL_CONNECTIONS, CrawlerConstants::Network::MAX_CONNECTIONS);
    curl_multi_setopt(multi_handle, CURLMOPT_MAX_HOST_CONNECTIONS, CrawlerConstants::Network::MAX_HOST_CONNECTIONS);
    curl_multi_setopt(multi_handle, CURLMOPT_PIPELINING, CURLPIPE_MULTIPLEX);
    
    // Set up global DNS cache
    CURLSH* share_handle = curl_share_init();
    curl_share_setopt(share_handle, CURLSHOPT_SHARE, CURL_LOCK_DATA_DNS);
    
    // Active requests management
    std::unordered_map<CURL*, std::unique_ptr<MultiRequestContext>> active_requests;
    const int MAX_CONCURRENT = CrawlerConstants::Queue::MAX_CONCURRENT_REQUESTS;
    
    // Batch processing
    std::vector<std::pair<std::string, std::string>> batch_buffer;
    batch_buffer.reserve(100);
    
    // Worker statistics & queue drain diagnostics
    int pages_processed = 0;
    int urls_dequeued = 0;
    int urls_skipped_blacklist = 0;
    int urls_skipped_robots = 0;
    int urls_skipped_rate_limit = 0;
    int successful_requests = 0;
    auto worker_start = std::chrono::steady_clock::now();
    auto last_queue_check = worker_start;
    
    std::cout << "üèÉ Worker " << worker_id << " starting with MAX_CONCURRENT=" << MAX_CONCURRENT << "\n";
    
    while (!stop_flag) {
        auto loop_start = std::chrono::steady_clock::now();
        
        // Log queue drain diagnostics every configured interval
        if (std::chrono::duration_cast<std::chrono::seconds>(loop_start - last_queue_check).count() >= 
            CrawlerConstants::Monitoring::WORKER_DIAGNOSTICS_INTERVAL_SECONDS) {
            size_t current_queue_size = url_frontier->size();
            std::cout << "üîç Worker " << worker_id << " diagnostics: "
                     << "Queue=" << current_queue_size 
                     << " | Dequeued=" << urls_dequeued
                     << " | Active=" << active_requests.size()
                     << " | Processed=" << pages_processed
                     << " | Skipped: BL=" << urls_skipped_blacklist 
                     << " R=" << urls_skipped_robots 
                     << " RL=" << urls_skipped_rate_limit << "\n";
            last_queue_check = loop_start;
        }
        
        // Add new requests up to the limit - Phase 2: with work stealing
        int attempts = 0;
        const int MAX_ATTEMPTS = CrawlerConstants::Queue::MAX_QUEUE_DRAIN_ATTEMPTS;
        while (active_requests.size() < MAX_CONCURRENT && !stop_flag && attempts < MAX_ATTEMPTS) {
            UrlInfo url_info("", 0.0f);
            attempts++;
            
            // Phase 2: Try multiple sources in priority order
            bool found_url = false;
            
            // 1. Try main priority queue first
            if (url_frontier->dequeue(url_info)) {
                found_url = true;
            }
            // 2. Try local work stealing queue
            else if (work_stealing_queue->pop_local(worker_id, url_info)) {
                found_url = true;
            }
            // 3. Try stealing from other workers
            else if (work_stealing_queue->try_steal(worker_id, url_info)) {
                found_url = true;
            }
            // 4. Try loading from sharded disk queue
            else {
                auto disk_urls = sharded_disk_queue->load_urls_from_disk(CrawlerConstants::Queue::SHARDED_DISK_LOAD_SIZE);
                if (!disk_urls.empty()) {                // Use first URL and re-queue the rest
                url_info = UrlInfo(disk_urls[0], CrawlerConstants::Priority::DISK_URL_PRIORITY, 0);
                    found_url = true;
                         // Re-queue remaining URLs
                for (size_t i = 1; i < disk_urls.size(); ++i) {
                    UrlInfo disk_url(disk_urls[i], CrawlerConstants::Priority::DISK_URL_PRIORITY, 0);
                        if (!url_frontier->enqueue(disk_url)) {
                            work_stealing_queue->push_local(worker_id, disk_url);
                        }
                    }
                }
            }
            
            if (!found_url) {
                static int empty_queue_count = 0;
                empty_queue_count++;
                if (empty_queue_count % CrawlerConstants::Monitoring::EMPTY_QUEUE_LOG_FREQUENCY == 1) {
                    std::cout << "‚ö†Ô∏è Worker " << worker_id << " exhausted all URL sources (count: " << empty_queue_count << ")\n";
                }
                break;
            }
            
            urls_dequeued++;
            
            std::string domain = UrlNormalizer::extract_domain(url_info.url);
            std::string path = UrlNormalizer::extract_path(url_info.url);
            
            // Skip blacklisted domains
            if (blacklist.is_blacklisted(domain)) {
                urls_skipped_blacklist++;
                continue;
            }
            
            // Fast-path for trusted domains - skip robots.txt entirely for speed
            bool is_trusted_domain = (domain.find("wikipedia.org") != std::string::npos ||
                                    domain.find("github.com") != std::string::npos ||
                                    domain.find("stackoverflow.com") != std::string::npos ||
                                    domain.find("httpbin.org") != std::string::npos ||
                                    domain.find("jsonplaceholder.typicode.com") != std::string::npos ||
                                    domain.find("arxiv.org") != std::string::npos ||
                                    domain.find("reddit.com") != std::string::npos);
            
            // Check robots.txt compliance only for untrusted domains
            if (!is_trusted_domain && !robots.is_allowed(domain, path)) {
                urls_skipped_robots++;
                continue;
            }
            
            // Apply rate limiting (non-blocking check) - with domain-specific queuing
            if (!limiter.can_request_now(domain)) {
                // Store domains that are rate limited in thread_local storage
                static thread_local std::unordered_map<std::string, std::queue<UrlInfo>> domain_queues;
                
                // Instead of re-queuing in the main frontier, store in domain-specific queue
                if (domain_queues[domain].size() < CrawlerConstants::Queue::DOMAIN_QUEUE_LIMIT) {
                    domain_queues[domain].push(url_info);
                } else {
                    // If domain queue is full, re-queue in main frontier
                    url_frontier->enqueue(url_info);
                }
                
                urls_skipped_rate_limit++;
                
                // Try to dequeue from a different domain that's not rate limited
                bool found_alt = false;
                for (auto& [d, q] : domain_queues) {
                    if (!q.empty() && limiter.can_request_now(d)) {
                        url_info = q.front();
                        q.pop();
                        domain = d;
                        path = UrlNormalizer::extract_path(url_info.url);
                        found_alt = true;
                        break;
                    }
                }
                
                if (!found_alt) {
                    continue; // Try next URL from different domain
                }
                // If we found a non-rate-limited URL, we'll proceed with it
            }
            
            // Create request context - temporarily disable domain-specific connections
            auto ctx = std::make_unique<MultiRequestContext>(url_info);
            successful_requests++;
            attempts = 0; // Reset attempts after successful request creation
            
            // Configure CURL handle for production use with speed optimizations
            curl_easy_setopt(ctx->curl_handle, CURLOPT_URL, ctx->url.c_str());
            curl_easy_setopt(ctx->curl_handle, CURLOPT_WRITEFUNCTION, hybrid_write_callback);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_WRITEDATA, &ctx->response_data);
            
            // Performance optimizations - balanced timeouts
            curl_easy_setopt(ctx->curl_handle, CURLOPT_TIMEOUT, CrawlerConstants::Network::TIMEOUT_SECONDS);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_CONNECTTIMEOUT, CrawlerConstants::Network::CONNECT_TIMEOUT_SECONDS);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_FOLLOWLOCATION, CrawlerConstants::Security::FOLLOW_REDIRECTS);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_MAXREDIRS, CrawlerConstants::Network::MAX_REDIRECTS);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_NOSIGNAL, CrawlerConstants::Security::THREAD_SAFE_MODE);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_TCP_NODELAY, CrawlerConstants::Security::TCP_NODELAY);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_TCP_KEEPALIVE, CrawlerConstants::Security::TCP_KEEPALIVE);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_2_0); // HTTP/2
            curl_easy_setopt(ctx->curl_handle, CURLOPT_BUFFERSIZE, CrawlerConstants::Network::BUFFER_SIZE);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_ACCEPT_ENCODING, CrawlerConstants::Headers::ACCEPT_ENCODING);
            
            // Production headers
            curl_easy_setopt(ctx->curl_handle, CURLOPT_USERAGENT, CrawlerConstants::Headers::USER_AGENT);
            
            // SSL settings for production
            curl_easy_setopt(ctx->curl_handle, CURLOPT_SSL_VERIFYPEER, CrawlerConstants::Security::SSL_VERIFY_PEER);
            curl_easy_setopt(ctx->curl_handle, CURLOPT_SSL_VERIFYHOST, CrawlerConstants::Security::SSL_VERIFY_HOST);
            
            // Add to multi handle
            CURLMcode mc = curl_multi_add_handle(multi_handle, ctx->curl_handle);
            if (mc == CURLM_OK) {
                limiter.record_request(domain);
                active_requests[ctx->curl_handle] = std::move(ctx);
            } else {
                std::cerr << "Failed to add handle to multi: " << curl_multi_strerror(mc) << "\n";
            }
        }
        
        // No active requests, sleep briefly
        if (active_requests.empty()) {
            std::this_thread::sleep_for(std::chrono::milliseconds(50));
            continue;
        }
        
        // Perform all transfers
        int running_handles = 0;
        CURLMcode mc = curl_multi_perform(multi_handle, &running_handles);
        if (mc != CURLM_OK) {
            std::cerr << "curl_multi_perform failed: " << curl_multi_strerror(mc) << "\n";
            break;
        }
        
        // Check for completed transfers
        CURLMsg* msg;
        int msgs_left;
        while ((msg = curl_multi_info_read(multi_handle, &msgs_left))) {
            if (msg->msg == CURLMSG_DONE) {
                CURL* curl = msg->easy_handle;
                auto it = active_requests.find(curl);
                
                if (it != active_requests.end()) {
                    auto& ctx = it->second;
                    
                    if (msg->data.result == CURLE_OK) {
                        long http_code = 0;
                        curl_off_t download_size = 0;
                        curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &http_code);
                        curl_easy_getinfo(curl, CURLINFO_SIZE_DOWNLOAD_T, &download_size);
                        
                        limiter.record_success(ctx->domain);
                        error_tracker.record_success(ctx->domain);
                        global_monitor.add_bytes(static_cast<long>(download_size));
                        
                        // Process successful response
                        if (http_code == CrawlerConstants::HttpStatus::OK && !ctx->response_data.empty()) {
                            // Validate content quality
                            if (ContentFilter::is_high_quality_content(ctx->response_data)) {
                                pages_processed++;
                                global_monitor.increment_pages();
                                
                                // Extract page metadata
                                std::string title = HtmlParser::extract_title(ctx->response_data);
                                
                                // Log page information
                                crawl_logger->log_page(ctx->url, title, http_code, ctx->url_info.depth,
                                                     ctx->domain, ctx->response_data.size(), ctx->start_time);
                                
                                // üåê ZERO-COST LANGUAGE DETECTION: Filter non-English pages
                                if (FastLanguageDetector::is_english_content(ctx->response_data, ctx->url)) {
                                    // Add to batch for file storage (only English content)
                                    batch_buffer.emplace_back(ctx->url, ctx->response_data);
                                    
                                    // Save batch when it reaches optimal size
                                    if (batch_buffer.size() >= CrawlerConstants::Storage::BATCH_SIZE) {
                                        file_storage->save_html_batch(batch_buffer);
                                        batch_buffer.clear();  // ‚úÖ Only clear after saving
                                    }
                                } else {
                                    // Skip non-English content - don't store it
                                    global_monitor.increment_filtered();
                                }
                                                                
                                // PHASE 2: Pipeline HTML processing instead of blocking here
                                if (ctx->url_info.depth < 5 && HtmlParser::is_html_content(ctx->response_data)) {
                                    // Queue HTML for asynchronous processing
                                    HtmlProcessingTask html_task(
                                        std::move(ctx->response_data), // Move to avoid copy
                                        ctx->url,
                                        ctx->domain,
                                        ctx->url_info.depth
                                    );
                                    
                                    if (!html_processing_queue->enqueue(std::move(html_task))) {
                                        // HTML queue full, process synchronously as fallback
                                        std::vector<std::string> links = AdaptiveLinkExtractor::extract_links_adaptive(
                                            ctx->response_data, ctx->url);
                                        
                                        for (const std::string& link : links) {
                                            std::string link_domain = UrlNormalizer::extract_domain(link);
                                            if (domain_manager->is_new_domain(link_domain)) {
                                                domain_manager->register_domain(link_domain, robots);
                                            }
                                        }
                                        
                                        int new_links_added = AdaptiveLinkExtractor::process_and_enqueue_links(
                                            links, ctx->url_info.depth, ctx->domain, worker_id);
                                        global_monitor.increment_links(new_links_added);
                                    }
                                }
                            }
                        } else if (http_code == CrawlerConstants::HttpStatus::TOO_MANY_REQUESTS || 
                                  http_code == CrawlerConstants::HttpStatus::SERVICE_UNAVAILABLE) {
                            // Handle rate limiting from server
                            limiter.throttle_domain(ctx->domain, CrawlerConstants::Network::THROTTLE_DURATION_SECONDS);
                        } else if (http_code >= CrawlerConstants::HttpStatus::BAD_REQUEST) {
                            crawl_logger->log_error(ctx->url, "HTTP " + std::to_string(http_code));
                        }
                    } else {
                        // Handle cURL errors
                        global_monitor.increment_errors();
                        limiter.record_failure(ctx->domain);
                        error_tracker.record_error(ctx->domain, msg->data.result);
                        
                        // Check if domain should be blacklisted
                        if (error_tracker.should_blacklist_domain(ctx->domain)) {
                            blacklist.add_temporary(ctx->domain);
                            std::cout << "Worker " << worker_id << " blacklisted domain: " << ctx->domain << std::endl;
                        }
                        
                        crawl_logger->log_error(ctx->url, curl_easy_strerror(msg->data.result));
                    }
                    
                    // Remove from multi handle and cleanup
                    curl_multi_remove_handle(multi_handle, curl);
                    active_requests.erase(it);
                }
            }
        }
        
        // Wait for activity with timeout
        if (running_handles > 0) {
            curl_multi_wait(multi_handle, nullptr, 0, 100, nullptr);
        }
        
        // Periodic progress reporting - reduced frequency for performance
        if (pages_processed % CrawlerConstants::Monitoring::PROGRESS_REPORT_FREQUENCY == 0 && pages_processed > 0) {
            auto now = std::chrono::steady_clock::now();
            auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(now - worker_start).count();
            if (elapsed > 0) {
                // Progress logged only every 500 pages for performance
            }
        }
    }
    
    // Cleanup remaining requests
    for (auto& [curl, ctx] : active_requests) {
        curl_multi_remove_handle(multi_handle, curl);
    }
    
    // Save any remaining batch (filter for English content)
    if (!batch_buffer.empty()) {
        // Filter English content from final batch
        std::vector<std::pair<std::string, std::string>> english_batch;
        for (const auto& [url, content] : batch_buffer) {
            if (FastLanguageDetector::is_english_content(content, url)) {
                english_batch.emplace_back(url, content);
            } else {
                global_monitor.increment_filtered();
            }
        }
        
        if (!english_batch.empty()) {
            file_storage->save_html_batch(english_batch);
        }
    }
    
    // Clean up shared handle and multi handle
    curl_share_cleanup(share_handle);
    curl_multi_cleanup(multi_handle);
    std::cout << "Multi-worker " << worker_id << " finished. Processed " << pages_processed << " pages.\n";
}

/**
 * üîß PHASE 2: DEDICATED HTML PROCESSING WORKER
 * Separates HTML parsing from network I/O for better pipeline efficiency
 */
void html_processing_worker(int worker_id, RobotsTxtCache& robots) {
    std::cout << "üîß HTML processor " << worker_id << " starting...\n";
    
    int links_processed = 0;
    int batches_processed = 0;
    auto worker_start = std::chrono::steady_clock::now();
    
    while (!stop_flag) {
        HtmlProcessingTask task("", "", "", 0);
        
        // Get next HTML processing task
        if (!html_processing_queue->dequeue(task)) {
            if (stop_flag) break;
            continue;
        }
        
        try {
            // Extract links using fast streaming parser
            std::vector<std::string> links = AdaptiveLinkExtractor::extract_links_adaptive(
                task.html, task.url);
            
            // Register new domains for robots.txt fetching
            for (const std::string& link : links) {
                std::string link_domain = UrlNormalizer::extract_domain(link);
                if (domain_manager->is_new_domain(link_domain)) {
                    domain_manager->register_domain(link_domain, robots);
                }
            }
            
            // Process and enqueue links with Phase 2 improvements
            int new_links_added = AdaptiveLinkExtractor::process_and_enqueue_links(
                links, task.depth, task.domain, worker_id);
            
            global_monitor.increment_links(new_links_added);
            links_processed += links.size();
            batches_processed++;
            
            // Periodic progress for HTML processors
            if (batches_processed % CrawlerConstants::Monitoring::PROGRESS_REPORT_FREQUENCY/5 == 0) {
                auto now = std::chrono::steady_clock::now();
                auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(now - worker_start).count();
                if (elapsed > 0) {
                    double rate = static_cast<double>(links_processed) / elapsed;
                    std::cout << "üîß HTML processor " << worker_id << ": " 
                             << batches_processed << " batches, " 
                             << links_processed << " links (" 
                             << std::fixed << std::setprecision(1) << rate << " links/s)\n";
                }
            }
            
        } catch (const std::exception& e) {
            std::cerr << "HTML processing error: " << e.what() << "\n";
        }
    }
    
    std::cout << "üîß HTML processor " << worker_id << " finished. Processed " 
             << batches_processed << " batches, " << links_processed << " total links.\n";
}

/**
 * üö® EMERGENCY SEED INJECTOR
 * Provides high-quality URLs when crawl queue gets too low
 */
class EmergencySeedInjector {
private:
    static std::vector<std::string> get_emergency_seeds() {
        return {
            // Fast, reliable endpoints
            "https://httpbin.org/links/100",
            "https://httpbin.org/range/200",
            "https://httpbin.org/html",
            "https://httpbin.org/json",
            "https://httpbin.org/xml",
            
            // Wikipedia - high link density
            "https://en.wikipedia.org/wiki/Special:Random",
            "https://en.wikipedia.org/wiki/List_of_programming_languages",
            "https://en.wikipedia.org/wiki/Computer_programming",
            "https://en.wikipedia.org/wiki/Software_engineering",
            "https://en.wikipedia.org/wiki/Data_science",
            "https://en.wikipedia.org/wiki/Machine_learning",
            "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "https://en.wikipedia.org/wiki/Web_development",
            "https://en.wikipedia.org/wiki/Database",
            "https://en.wikipedia.org/wiki/Computer_science",
            
            // StackOverflow - good for technical content
            "https://stackoverflow.com/questions",
            "https://stackoverflow.com/questions/tagged/python",
            "https://stackoverflow.com/questions/tagged/javascript",
            "https://stackoverflow.com/questions/tagged/web-scraping",
            "https://stackoverflow.com/questions/tagged/machine-learning",
            "https://stackoverflow.com/questions/tagged/database",
            
            // GitHub - developer content
            "https://github.com/trending",
            "https://github.com/trending/python",
            "https://github.com/trending/javascript",
            "https://github.com/topics/machine-learning",
            "https://github.com/topics/artificial-intelligence",
            "https://github.com/topics/web-development",
            
            // News and tech sites
            "https://news.ycombinator.com",
            "https://news.ycombinator.com/newest",
            "https://news.ycombinator.com/best",
            
            // Academic sources
            "https://arxiv.org/list/cs.AI/recent",
            "https://arxiv.org/list/cs.LG/recent",
            "https://scholar.google.com/citations?view_op=search_venues&hl=en&vq=computer+science"
        };
    }
    
public:
    static bool inject_emergency_seeds(int& injection_count, 
                                       const int max_injections = CrawlerConstants::ErrorHandling::MAX_EMERGENCY_INJECTIONS) {
        if (injection_count >= max_injections) {
            return false;
        }
        
        auto seeds = get_emergency_seeds();
        int injected = 0;
        
        for (const auto& seed : seeds) {
            float priority = CrawlerConstants::Priority::EMERGENCY_SEED_PRIORITY; // Very high priority for emergency seeds
            UrlInfo seed_info(seed, priority, 0);
            if (url_frontier->enqueue(seed_info)) {
                injected++;
            }
        }
        
        injection_count++;
        std::cout << "üö® Emergency injection #" << injection_count << ": Added " 
                 << injected << "/" << seeds.size() << " emergency seeds\n";
        
        return true;
    }
};
/**
 * üìä ENHANCED MONITORING THREAD with Always-On Queue & Speed Logging
 */
void enhanced_monitoring_thread() {
    std::cout << "üìä Starting continuous queue & speed monitoring...\n";
    
    auto last_stats = std::chrono::steady_clock::now();
    auto last_cleanup = std::chrono::steady_clock::now();
    auto monitoring_start = std::chrono::steady_clock::now();
    
    static int emergency_injection_count = 0;
    static int low_queue_warnings = 0;
    
    // Initial queue status - log immediately at startup
    size_t initial_queue_size = url_frontier->size();
    size_t initial_disk_queue_size = sharded_disk_queue->get_total_disk_queue_size();
    size_t initial_work_stealing_size = work_stealing_queue->total_size();
    std::cout << "üîç STARTUP QUEUE STATUS:\n";
    std::cout << "   Memory Queue: " << initial_queue_size << " URLs\n";
    std::cout << "   Sharded Disk Queue: " << initial_disk_queue_size << " URLs\n";
    std::cout << "   Work Stealing Queue: " << initial_work_stealing_size << " URLs\n";
    std::cout << "   HTML Processing Queue: " << html_processing_queue->size() << " tasks\n";
    std::cout << "   Total Available: " << (initial_queue_size + initial_disk_queue_size + initial_work_stealing_size) << " URLs\n\n";
    
    while (!stop_flag) {
        std::this_thread::sleep_for(std::chrono::seconds(CrawlerConstants::Monitoring::QUEUE_CHECK_INTERVAL_SECONDS));
        
        auto now = std::chrono::steady_clock::now();
        auto elapsed_seconds = std::chrono::duration_cast<std::chrono::seconds>(now - monitoring_start).count();
        
        size_t queue_size = url_frontier->size();
        size_t disk_queue_size = sharded_disk_queue->get_total_disk_queue_size();
        size_t work_stealing_size = work_stealing_queue->total_size();
        size_t html_queue_size = html_processing_queue->size();
        double current_rate = global_monitor.get_crawl_rate();
        size_t total_processed = global_monitor.get_total_pages();
        
        // ALWAYS LOG QUEUE STATUS & SPEED (every 5 seconds) - Phase 2 enhanced
        std::cout << "[" << std::setw(4) << elapsed_seconds << "s] "
                 << "Main: " << std::setw(4) << queue_size 
                 << " | Disk: " << std::setw(4) << disk_queue_size
                 << " | Work: " << std::setw(3) << work_stealing_size
                 << " | HTML: " << std::setw(3) << html_queue_size
                 << " | Speed: " << std::fixed << std::setprecision(1) << std::setw(6) << current_rate << " p/s"
                 << " | Total: " << std::setw(6) << total_processed << "\n";
        
        // Print detailed statistics every configured interval
        if (std::chrono::duration_cast<std::chrono::seconds>(now - last_stats).count() >= 
            CrawlerConstants::Monitoring::DETAILED_STATS_INTERVAL_SECONDS) {
            std::cout << "\nüìä DETAILED STATS (15s interval):\n";
            global_monitor.print_stats(queue_size, 0);
            
            // Performance indicators with more granular feedback
            if (current_rate >= CrawlerConstants::Performance::TARGET_PAGES_PER_SECOND) {
                std::cout << "üöÄ TARGET ACHIEVED: " << std::fixed << std::setprecision(1) 
                         << current_rate << " pages/sec\n";
            } else if (current_rate >= CrawlerConstants::Performance::HIGH_PERFORMANCE_THRESHOLD) {
                std::cout << "‚ö° High Performance: " << std::fixed << std::setprecision(1) 
                         << current_rate << " pages/sec\n";
            } else if (current_rate >= CrawlerConstants::Performance::GOOD_PERFORMANCE_THRESHOLD) {
                std::cout << "üî• Good Performance: " << std::fixed << std::setprecision(1) 
                         << current_rate << " pages/sec\n";
            } else if (current_rate >= CrawlerConstants::Performance::MODERATE_PERFORMANCE_THRESHOLD) {
                std::cout << "‚ö†Ô∏è Moderate Performance: " << std::fixed << std::setprecision(1) 
                         << current_rate << " pages/sec\n";
            } else if (current_rate >= CrawlerConstants::Performance::LOW_PERFORMANCE_THRESHOLD) {
                std::cout << "üêå Low Performance: " << std::fixed << std::setprecision(1) 
                         << current_rate << " pages/sec\n";
            } else {
                std::cout << "üî¥ Very Low Performance: " << std::fixed << std::setprecision(1) 
                         << current_rate << " pages/sec\n";
            }
            
            std::cout << "\n";
            std::cout.flush();
            last_stats = now;
        }
        
        // üîÑ PHASE 2: ENHANCED QUEUE MANAGEMENT with sharded disk and work stealing
        
        // 1. Refill from sharded disk when main queue gets low
        if (queue_size < CrawlerConstants::Queue::REFILL_THRESHOLD && disk_queue_size > 0) {
            auto loaded_urls = sharded_disk_queue->load_urls_from_disk(CrawlerConstants::Queue::REFILL_THRESHOLD);
            int refilled = 0;
            
            for (const auto& url : loaded_urls) {
                float priority = CrawlerConstants::Priority::DISK_URL_PRIORITY;
                UrlInfo url_info(url, priority, 0);
                if (url_frontier->enqueue(url_info)) {
                    refilled++;
                }
            }
            
            if (refilled > 0) {
                std::cout << "‚úÖ Loaded " << refilled << " URLs from sharded disk (Main queue was " << queue_size << ")\n";
            }
        }
        
        // 2. Periodic cleanup of empty disk shards
        if (elapsed_seconds % CrawlerConstants::Monitoring::CLEANUP_INTERVAL_SECONDS == 0) { // Every minute
            sharded_disk_queue->cleanup_empty_shards();
        }
        
        // 2. Emergency seed injection when critically low - more aggressive
        if (queue_size < CrawlerConstants::Queue::LOW_QUEUE_THRESHOLD && 
            current_rate < CrawlerConstants::Performance::SHUTDOWN_RATE_THRESHOLD) {
            low_queue_warnings++;
            
            if (low_queue_warnings >= CrawlerConstants::ErrorHandling::LOW_QUEUE_WARNING_THRESHOLD) {
                if (EmergencySeedInjector::inject_emergency_seeds(emergency_injection_count)) {
                    low_queue_warnings = 0; // Reset warnings after injection
                }
            }
        } else {
            low_queue_warnings = 0; // Reset warnings when queue recovers
        }
        
        // 3. Auto-shutdown conditions - Phase 2 enhanced
        size_t total_urls_available = queue_size + disk_queue_size + work_stealing_size;
        if (total_urls_available < CrawlerConstants::Queue::CRITICAL_QUEUE_THRESHOLD && 
            current_rate < CrawlerConstants::Performance::VERY_LOW_PERFORMANCE_THRESHOLD) {
            static int shutdown_warnings = 0;
            shutdown_warnings++;
            
            std::cout << "üõë Shutdown condition detected: Total URLs=" << total_urls_available
                     << " (Main=" << queue_size << ", Disk=" << disk_queue_size 
                     << ", Work=" << work_stealing_size << "), Rate=" << current_rate 
                     << " (warning #" << shutdown_warnings << "/" 
                     << CrawlerConstants::ErrorHandling::SHUTDOWN_WARNING_THRESHOLD << ")\n";
            
            if (shutdown_warnings >= CrawlerConstants::ErrorHandling::SHUTDOWN_WARNING_THRESHOLD) {
                std::cout << "üèÅ Triggering graceful shutdown - no more URLs to crawl\n";
                stop_flag = true;
            }
        }
        
        // Safety timeout (configurable)
        auto total_elapsed = std::chrono::duration_cast<std::chrono::minutes>(now - last_cleanup).count();
        if (total_elapsed >= CrawlerConstants::Monitoring::SAFETY_TIMEOUT_MINUTES) {
            std::cout << "‚è∞ Safety timeout reached (" << CrawlerConstants::Monitoring::SAFETY_TIMEOUT_MINUTES 
                     << " minutes). Shutting down...\n";
            stop_flag = true;
        }
    }
}

void signal_handler(int) {
    stop_flag = true;
    std::cout << "\nReceived shutdown signal. Gracefully shutting down hybrid crawler...\n";
}

int main(int argc, char* argv[]) {
    std::cout << "üöÄ HYBRID SPEED CRAWLER - Production-Ready Ultimate Performance\n";
    std::cout << "================================================================\n";
    
    // Initialize cURL globally
    curl_global_init(CURL_GLOBAL_DEFAULT);
    
    // Parse command line arguments - Phase 2: Optimal configuration
    int max_threads = std::min(CrawlerConstants::Workers::DEFAULT_MAX_THREADS, 
                              (int)std::thread::hardware_concurrency());
    int max_depth = CrawlerConstants::Queue::DEFAULT_MAX_DEPTH;
    int max_queue_size = CrawlerConstants::Queue::DEFAULT_MAX_QUEUE_SIZE;
    
    if (argc > 1) max_threads = std::atoi(argv[1]);
    if (argc > 2) max_depth = std::atoi(argv[2]);
    if (argc > 3) max_queue_size = std::atoi(argv[3]);
    
    // Phase 2: Calculate optimal worker distribution
    int network_workers = max_threads;
    int html_workers = std::max(CrawlerConstants::Workers::MIN_HTML_WORKERS, 
                               max_threads / CrawlerConstants::Workers::HTML_WORKER_RATIO);
    
    std::cout << "Configuration - Phase 2 Enhanced:\n";
    std::cout << "- Network workers: " << network_workers << "\n";
    std::cout << "- HTML processors: " << html_workers << "\n";
    std::cout << "- Max crawl depth: " << max_depth << "\n";
    std::cout << "- Max queue size: " << max_queue_size << "\n";
    std::cout << "- Target performance: " << CrawlerConstants::Performance::TARGET_PAGES_PER_SECOND << "+ pages/sec\n";
    std::cout << "- Phase 2 features: Sharded disk, Work stealing, HTML pipeline\n";
    std::cout << "================================================================\n\n";
    
    // Initialize Phase 2 global components
    url_frontier = std::make_unique<UrlFrontier>();
    url_frontier->set_max_depth(max_depth);
    url_frontier->set_max_queue_size(max_queue_size);
    
    crawl_logger = std::make_unique<CrawlLogger>(
        CrawlerConstants::Paths::DB_PATH,
        CrawlerConstants::Paths::LOG_PATH
    );
    
    file_storage = std::make_unique<FileStorageManager>(CrawlerConstants::Paths::RAW_DATA_PATH);
    
    // Phase 2: Initialize enhanced components
    sharded_disk_queue = std::make_unique<ShardedDiskQueue>(CrawlerConstants::Paths::SHARDED_DISK_PATH);
    html_processing_queue = std::make_unique<HtmlProcessingQueue>();
    work_stealing_queue = std::make_unique<WorkStealingQueue>(network_workers);
    domain_manager = std::make_unique<DynamicDomainManager>();
    
    // Initialize other components
    RobotsTxtCache robots;
    RateLimiter limiter;
    DomainBlacklist blacklist;
    ErrorTracker error_tracker;
    
    // Load blacklist if available
    blacklist.load_from_file(CrawlerConstants::Paths::BLACKLIST_PATH);
    
    // High-quality seed URLs optimized for speed and variety - EXPANDED
    const std::vector<std::string> seed_urls = {
        // Art & Culture
        "https://www.metmuseum.org/art/collection",
        "https://www.tate.org.uk/art/artists",
        "https://artsandculture.google.com/category/artist",
        "https://www.nga.gov/collection/artists.html",
        "https://www.britishmuseum.org/collection",

        // History
        "https://www.history.com/topics",
        "https://www.archives.gov/research/catalog",
        "https://www.britannica.com/list/browse/history",
        "https://www.worldhistory.org/timeline/",
        "https://www.si.edu/collections/history-culture",

        // Science & Nature
        "https://www.nature.com/subjects",
        "https://www.science.org/toc/science/current",
        "https://www.nasa.gov/topics/earth/index.html",
        "https://www.nhm.ac.uk/discover.html",
        "https://ocean.si.edu/ocean-life",

        // Literature & Philosophy
        "https://www.poetryfoundation.org/poets",
        "https://plato.stanford.edu/contents.html",
        "https://www.gutenberg.org/browse/scores/top",
        "https://www.literaryhub.com/category/features",
        "https://iep.utm.edu/browse/",

        // Geography & Travel
        "https://www.nationalgeographic.com/maps",
        "https://whc.unesco.org/en/list/",
        "https://www.lonelyplanet.com/articles",
        "https://www.cia.gov/the-world-factbook/",
        "https://www.worldtravelguide.net/features/",

        // Technology & Engineering
        "https://spectrum.ieee.org/topic/artificial-intelligence/",
        "https://www.technologyreview.com/category/ai/",
        "https://www.digitaltrends.com/computing/",
        "https://www.engineering.com/",
        "https://www.sciencedaily.com/news/computers_math/computer_science/",

        // Health & Medicine
        "https://www.who.int/news-room/feature-stories",
        "https://www.mayoclinic.org/diseases-conditions/index",
        "https://www.nih.gov/health-information",
        "https://www.healthline.com/nutrition",
        "https://www.cdc.gov/ncezid/",

        // Economics & Business
        "https://www.economist.com/latest-updates",
        "https://www.bloomberg.com/markets",
        "https://www.mckinsey.com/featured-insights",
        "https://www.forbes.com/innovation/",
        "https://www.worldbank.org/en/topic",

        // Education & Reference
        "https://www.khanacademy.org/library",
        "https://ocw.mit.edu/courses/",
        "https://www.coursera.org/courses",
        "https://www.refdesk.com/facts.html",
        "https://www.merriam-webster.com/word-of-the-day",

        // Miscellaneous Knowledge
        "https://www.bonappetit.com/recipes",          // Food
        "https://www.espn.com/olympics/sports/",       // Sports
        "https://www.apa.org/topics",                  // Psychology
        "https://www.ipcc.ch/reports/",                // Environment
        "https://www.ethnologue.com/browse/languages", // Linguistics

        // High‚ÄëDensity Aggregators
        "https://www.reddit.com/r/todayilearned/top/",
        "https://www.bbc.co.uk/sounds/categories",
        "https://www.curlie.org/",
        "https://www.wikihow.com/Main-Page",
        "https://www.brainpickings.org/archive/",

        // Wikipedia (high link density)
        "https://en.wikipedia.org/wiki/Special:Random",
        "https://en.wikipedia.org/wiki/List_of_programming_languages",
        "https://en.wikipedia.org/wiki/Computer_programming",
        "https://en.wikipedia.org/wiki/Software_engineering",
        "https://en.wikipedia.org/wiki/Data_science",
        "https://en.wikipedia.org/wiki/Machine_learning",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Web_development",
        "https://en.wikipedia.org/wiki/Database",
        "https://en.wikipedia.org/wiki/Computer_science",

        // StackOverflow (technical content)
        "https://stackoverflow.com/questions",
        "https://stackoverflow.com/questions/tagged/python",
        "https://stackoverflow.com/questions/tagged/javascript",
        "https://stackoverflow.com/questions/tagged/web-scraping",
        "https://stackoverflow.com/questions/tagged/machine-learning",
        "https://stackoverflow.com/questions/tagged/database",

        // GitHub (developer content)
        "https://github.com/trending",
        "https://github.com/trending/python",
        "https://github.com/trending/javascript",
        "https://github.com/topics/machine-learning",
        "https://github.com/topics/artificial-intelligence",
        "https://github.com/topics/web-development",

        // News & Tech
        "https://news.ycombinator.com",
        "https://news.ycombinator.com/newest",
        "https://news.ycombinator.com/best",

        // Academic Sources
        "https://arxiv.org/list/cs.AI/recent",
        "https://arxiv.org/list/cs.LG/recent",
        "https://scholar.google.com/citations?view_op=search_venues&hl=en&vq=computer+science",

        // Reliable Testing Endpoints
        "https://httpbin.org/stream/100",       // streaming
        "https://httpbin.org/delay/5",          // timeout test
        "https://httpbin.org/status/418",       // edge HTTP codes
        "https://httpbin.org/bytes/1024",       // binary data
        "https://httpbin.org/user-agent"        // header echo
    };
    
    // Seed the frontier
    int successfully_seeded = 0;
    for (const auto& url : seed_urls) {
        float priority = ContentFilter::calculate_priority(url, 0);
        UrlInfo seed_info(url, priority, 0);
        if (url_frontier->enqueue(seed_info)) {
            successfully_seeded++;
        }
        
        // Pre-fetch robots.txt for seed domains
        std::string domain = UrlNormalizer::extract_domain(url);
        robots.fetch_and_cache(domain);
    }
    
    std::cout << "‚úÖ Seeded hybrid crawler with " << successfully_seeded << "/" << seed_urls.size() << " URLs\n";
    
    // Check initial queue status after seeding
    size_t post_seed_queue_size = url_frontier->size();
    size_t post_seed_disk_size = sharded_disk_queue->get_total_disk_queue_size();
    std::cout << "üìä POST-SEED QUEUE STATUS:\n";
    std::cout << "   Memory Queue: " << post_seed_queue_size << " URLs\n";
    std::cout << "   Sharded Disk Queue: " << post_seed_disk_size << " URLs\n";
    std::cout << "   Work Stealing Queue: " << work_stealing_queue->total_size() << " URLs\n";
    std::cout << "   HTML Processing Queue: " << html_processing_queue->size() << " tasks\n";
    
    // Register signal handler
    std::signal(SIGINT, signal_handler);
    std::signal(SIGTERM, signal_handler);
    
    // Start Phase 2: Network workers + HTML processors
    std::vector<std::thread> network_workers_threads;
    std::vector<std::thread> html_workers_threads;
    
    network_workers_threads.reserve(network_workers);
    html_workers_threads.reserve(html_workers);
    
    std::cout << "üöÄ Starting Phase 2 workers:\n";
    std::cout << "   Network workers: " << network_workers << "\n";
    std::cout << "   HTML processors: " << html_workers << "\n";
    
    // Start network workers
    for (int i = 0; i < network_workers; ++i) {
        network_workers_threads.emplace_back([i, &robots, &limiter, &blacklist, &error_tracker]() {
            multi_crawler_worker(i, std::ref(robots), std::ref(limiter),
                                std::ref(blacklist), std::ref(error_tracker));
        });
    }
    
    // Start HTML processing workers
    for (int i = 0; i < html_workers; ++i) {
        html_workers_threads.emplace_back([i, &robots]() {
            html_processing_worker(i, std::ref(robots));
        });
    }
    
    // Give workers a moment to start, then check queue drain
    std::this_thread::sleep_for(std::chrono::seconds(2));
    std::cout << "üìä POST-WORKER START STATUS (after 2s):\n";
    std::cout << "   Memory Queue: " << url_frontier->size() << " URLs\n";
    std::cout << "   Sharded Disk Queue: " << sharded_disk_queue->get_total_disk_queue_size() << " URLs\n";
    std::cout << "   Work Stealing Queue: " << work_stealing_queue->total_size() << " URLs\n";
    std::cout << "   HTML Processing Queue: " << html_processing_queue->size() << " tasks\n\n";
    
    // Start enhanced monitoring thread
    std::thread monitor_thread(enhanced_monitoring_thread);
    
    std::cout << "üöÄ Phase 2 Enhanced Crawler started!\n";
    std::cout << "   Network pipeline: " << network_workers << " workers\n";
    std::cout << "   HTML pipeline: " << html_workers << " processors\n";
    std::cout << "   Target: 300+ pages/sec with pipelined processing\n";
    std::cout << "Press Ctrl+C to gracefully shutdown\n\n";
    
    // Wait for Phase 2 workers completion
    for (auto& worker : network_workers_threads) {
        if (worker.joinable()) {
            worker.join();
        }
    }
    
    // Shutdown HTML processing pipeline
    html_processing_queue->shutdown();
    for (auto& worker : html_workers_threads) {
        if (worker.joinable()) {
            worker.join();
        }
    }
    
    if (monitor_thread.joinable()) {
        monitor_thread.join();
    }
    
    std::cout << "\nüéØ FINAL PHASE 2 CRAWLER RESULTS\n";
    std::cout << "===================================\n";
    global_monitor.print_stats(url_frontier->size(), 0);
   // error_tracker.print_stats();
    
    // Ultra parser performance stats (always shown now)
    UltraParser::UltraHTMLParser ultra_parser;
    ultra_parser.print_performance_stats();
    
    // Phase 2 final statistics
    std::cout << "üìä PHASE 2 QUEUE FINAL STATS:\n";
    std::cout << "   Memory Queue: " << url_frontier->size() << " URLs remaining\n";
    std::cout << "   Sharded Disk Queue: " << sharded_disk_queue->get_total_disk_queue_size() << " URLs remaining\n";
    std::cout << "   Work Stealing Queue: " << work_stealing_queue->total_size() << " URLs remaining\n";
    std::cout << "   HTML Processing Queue: " << html_processing_queue->size() << " tasks remaining\n";
    
    double final_rate = global_monitor.get_crawl_rate();
    if (final_rate >= 300.0) {
        std::cout << "‚úÖ SUCCESS: Achieved " << std::fixed << std::setprecision(1) 
                 << final_rate << " pages/sec (Target: 300+)\n";
    } else {
        std::cout << "üìä Performance: " << std::fixed << std::setprecision(1) 
                 << final_rate << " pages/sec\n";
    }
    
    // Phase 2 cleanup
    crawl_logger->flush();
    crawl_logger.reset();
    url_frontier.reset();
    file_storage.reset();
    sharded_disk_queue.reset();
    html_processing_queue.reset();
    work_stealing_queue.reset();
    domain_manager.reset();
    
    curl_global_cleanup();
    
    std::cout << "üèÅ Phase 2 Enhanced Crawler shutdown complete.\n";
    return 0;
}
